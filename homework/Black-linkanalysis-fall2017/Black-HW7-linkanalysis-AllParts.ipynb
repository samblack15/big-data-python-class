{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Below is my code for the crawl spider. It crawls the pages of the sports website bleacherreport.com\n",
    "#To run the spider, cd into the bleacherreport parent directory and run the command: 'scrapy crawl sports -o data.csv -t csv\",\n",
    "#Which will save the resulting output to a csv file called 'data.csv'\n",
    "#Created using the guide 'https://medium.com/python-pandemonium/develop-your-first-web-crawler-in-python-scrapy-6b2ee4baf954'\n",
    "\n",
    "import scrapy\n",
    "from scrapy.spiders import CrawlSpider, Rule\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from scrapy.selector import Selector\n",
    "from scrapy.http import Request\n",
    "from scrapy.exceptions import CloseSpider\n",
    "\n",
    "class BleacherreportItem(scrapy.Item):\n",
    "    parent_url = scrapy.Field() #Every page that is scraped saves the following information: the url and the url of the \n",
    "    url = scrapy.Field() #The page that referred to it\n",
    "\n",
    "\n",
    "class SportsSpider(CrawlSpider):\n",
    "    name = 'sports'\n",
    "    allowed_domains = ['www.bleacherreport.com']\n",
    "    start_urls = ['http://www.bleacherreport.com/']\n",
    "    \n",
    "    rules = (\n",
    "    Rule(LinkExtractor(allow=(), deny = \"users\"), #The spider will crawl every page, except those that have the word 'users'\n",
    "         callback=\"parse_item\", #in them. BleacherReport allows users to create profiles, and we're not interested in having\n",
    "         follow=True), #the spider crawl through all of the many profiles that exist.\n",
    "    )\n",
    "\n",
    "    page_count = 0\n",
    "\n",
    "    def start_requests(self):\n",
    "        requests = []\n",
    "        for start_url in self.start_urls:\n",
    "            requests.append(Request(url=start_url, headers={'Referer': start_url}))\n",
    "        return requests\n",
    "\n",
    "    def parse_item(self, response):\n",
    "        if self.page_count > 30000: ##30,000 page maximum\n",
    "            raise CloseSpider('Have scraped a sufficient number of pages')\n",
    "        else:\n",
    "            self.page_count += 1\n",
    "        if self.page_count % 1000 == 0:\n",
    "            print(self.page_count)\n",
    "            print('URL: ' + url)\n",
    "            print('Parent URL: ' + parent_url)\n",
    "        url = response.url\n",
    "        parent_url = response.request.headers.get('Referer', None).decode('utf-8')\n",
    "        item = BleacherreportItem()\n",
    "        item['parent_url'] = parent_url\n",
    "        item['url'] = url\n",
    "        yield item\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of rows in this dataframe is 18178\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>parent_url</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://www.bleacherreport.com/mobile</td>\n",
       "      <td>http://www.bleacherreport.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>http://www.bleacherreport.com/golf</td>\n",
       "      <td>http://www.bleacherreport.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.bleacherreport.com/college-basketball</td>\n",
       "      <td>http://www.bleacherreport.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>http://www.bleacherreport.com/tennis</td>\n",
       "      <td>http://www.bleacherreport.com/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://www.bleacherreport.com/nascar</td>\n",
       "      <td>http://www.bleacherreport.com/</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                url  \\\n",
       "0              http://www.bleacherreport.com/mobile   \n",
       "1                http://www.bleacherreport.com/golf   \n",
       "2  http://www.bleacherreport.com/college-basketball   \n",
       "3              http://www.bleacherreport.com/tennis   \n",
       "4              http://www.bleacherreport.com/nascar   \n",
       "\n",
       "                       parent_url  \n",
       "0  http://www.bleacherreport.com/  \n",
       "1  http://www.bleacherreport.com/  \n",
       "2  http://www.bleacherreport.com/  \n",
       "3  http://www.bleacherreport.com/  \n",
       "4  http://www.bleacherreport.com/  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Load data\n",
    "\n",
    "data = pd.read_csv('data.csv')\n",
    "\n",
    "print('The number of rows in this dataframe is ' + str(len(data)))\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a dictionary that will map each page to the index that it will be associated with in the connectivity graph\n",
    "#and stochastic matrix\n",
    "\n",
    "indexes = {}\n",
    "j = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if data['parent_url'][i] not in indexes:\n",
    "        indexes[data['parent_url'][i]] = j\n",
    "        j += 1\n",
    "    if data['url'][i] not in indexes:\n",
    "        indexes[data['url'][i]] = j\n",
    "        j += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the connectivity graph. A '0' means that there is no link from page A to page B, while a '1' menas that there is\n",
    "#a link from page A to page B\n",
    "\n",
    "connectivity_graph = np.zeros((len(indexes), len(indexes)))\n",
    "for i in range(len(data)):\n",
    "    parent_url_index = indexes[data['parent_url'][i]]\n",
    "    url_index = indexes[data['url'][i]]\n",
    "    connectivity_graph[parent_url_index][url_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector with each value being the total number of outgoing links for a particular page.\n",
    "# Connectivity matrix will be element-wise divided by this vector to create the stochastic matrix\n",
    "total_outgoing_links = np.sum(connectivity_graph,axis = 1).reshape(len(connectivity_graph), 1)\n",
    "\n",
    "#Replace all zeros with another value so that there are no divide by zero errors\n",
    "total_outgoing_links[total_outgoing_links == 0] = 1\n",
    "\n",
    "stochastic_matrix = connectivity_graph/total_outgoing_links\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the pagerank algorithm. It's input is the stochastic matrix that was just created\n",
    "#Aglorithm retrieved from 'https://cs7083.wordpress.com/2013/01/31/demystifying-the-pagerank-and-hits-algorithms/'\n",
    "\n",
    "def pagerank(H):\n",
    "    n= len(H)\n",
    "    w = np.zeros(n)\n",
    "    rho = 1./n * np.ones(n);\n",
    "    for i in range(n):\n",
    "        if np.multiply.reduce(H[i]== np.zeros(n)):\n",
    "            w[i] = 1\n",
    "    newH = H + np.outer((1./n * w),np.ones(n))\n",
    " \n",
    "    theta=0.85\n",
    "    G = (theta * newH) + ((1-theta) * np.outer(1./n * np.ones(n), np.ones(n)))\n",
    "    for j in range(10):\n",
    "        rho = np.dot(rho,G)\n",
    "    return rho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Create the pagerank vector. The value of index i in the vector is the relative importance compared with the other pages\n",
    "#of the page associated with index i in the 'indexes' dictionary\n",
    "\n",
    "pagerank_rankings = pagerank(stochastic_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 pagerank indexes are [11949 12242 12238 11804 11924]\n"
     ]
    }
   ],
   "source": [
    "#Find the 5 indexes with the greatest pagerank value.\n",
    "\n",
    "top_5_pagerank_indexes = pagerank_rankings.argsort()[-5:][::-1]\n",
    "print('The top 5 pagerank indexes are ' + str(top_5_pagerank_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an inversed dictionary that maps the indexes to the pages, so that each index can be easily looked up\n",
    "\n",
    "inv_indexes = {v: k for k, v in indexes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 pages according to the pagerank algorithm are: \n",
      "1.) http://www.bleacherreport.com/wwe-fastlane/archives\n",
      "2.) http://www.bleacherreport.com/san-antonio-stars/archives\n",
      "3.) http://www.bleacherreport.com/football/archives\n",
      "4.) http://www.bleacherreport.com/articles/2740848-ronda-rouseys-rumored-wrestlemania-bout-would-be-biggest-wwe-womens-match-ever\n",
      "5.) http://www.bleacherreport.com/wwe-fastlane\n"
     ]
    }
   ],
   "source": [
    "#Look up each index in the inversed dictionary\n",
    "\n",
    "top_5_pagerank_pages = []\n",
    "\n",
    "for index in top_5_pagerank_indexes:\n",
    "    top_5_pagerank_pages.append(inv_indexes[index])\n",
    "    \n",
    "print('The top 5 pages according to the pagerank algorithm are: ')\n",
    "for page in top_5_pagerank_pages:\n",
    "    print (str(top_5_pagerank_pages.index(page) + 1) + '.) ' + page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now define the hits algorithm. It outputs 2 rankings, the Hits Authority ranking,\n",
    "#and the Hits Hub ranking. The algorithm's input is the connectivity graph.\n",
    "#Algorithm retrieved from 'https://cs7083.wordpress.com/2013/01/31/demystifying-the-pagerank-and-hits-algorithms/'\n",
    "\n",
    "\n",
    "def hits(A):\n",
    "    n= len(A)\n",
    "    Au= np.dot(np.transpose(A),A)\n",
    "    Hu = np.dot(A,np.transpose(A))\n",
    "    a = np.ones(n); h = np.ones(n)\n",
    "    for j in range(5):\n",
    "        a = np.dot(a,Au)\n",
    "        a = a/sum(a)\n",
    "        h = np.dot(h,Hu)\n",
    "        h = h/ sum(h)\n",
    "    return a, h\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Data set is large so this hits algorithm takes a while to run\n",
    "\n",
    "hits_authority_rankings, hits_hub_rankings = hits(connectivity_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 Hits Authority indexes are [6242 7255 7257 7258 7259]\n",
      "The top 5 Hits Hub ranking indexes are [ 5046 11107 14555    25 11452]\n"
     ]
    }
   ],
   "source": [
    "#Find the 5 indexes with the greatest values in each of the 2 rankings:\n",
    "\n",
    "top_5_hits_authority_indexes = hits_authority_rankings.argsort()[-5:][::-1]\n",
    "top_5_hits_hub_indexes = hits_hub_rankings.argsort()[-5:][::-1]\n",
    "\n",
    "print('The top 5 Hits Authority indexes are ' + str(top_5_hits_authority_indexes))\n",
    "print('The top 5 Hits Hub ranking indexes are ' + str(top_5_hits_hub_indexes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The top 5 pages according to the hits authority ranking are: \n",
      "1.) http://www.bleacherreport.com/houston-dynamo\n",
      "2.) http://www.bleacherreport.com/armenia-national-football\n",
      "3.) http://www.bleacherreport.com/argentina\n",
      "4.) http://www.bleacherreport.com/andorra-national-football\n",
      "5.) http://www.bleacherreport.com/american-samoa-national-football\n",
      "The top 5 pages according to the hits hub ranking are: \n",
      "1.) http://www.bleacherreport.com/world-football/teams\n",
      "2.) http://www.bleacherreport.com/mma/teams\n",
      "3.) http://www.bleacherreport.com/college-basketball/teams\n",
      "4.) http://www.bleacherreport.com/college-football/teams\n",
      "5.) http://www.bleacherreport.com/pro-wrestling/teams\n"
     ]
    }
   ],
   "source": [
    "#Find each of the indexes's respective page using the 'inv_indexes' dictionary from before\n",
    "\n",
    "top_5_hits_authority_pages = []\n",
    "top_5_hits_hub_pages = []\n",
    "\n",
    "\n",
    "for index in top_5_hits_authority_indexes:\n",
    "    top_5_hits_authority_pages.append(inv_indexes[index])\n",
    "    \n",
    "\n",
    "for index in top_5_hits_hub_indexes:\n",
    "    top_5_hits_hub_pages.append(inv_indexes[index])\n",
    "    \n",
    "print('The top 5 pages according to the hits authority ranking are: ')\n",
    "for page in top_5_hits_authority_pages:\n",
    "    print (str(top_5_hits_authority_pages.index(page) + 1) + '.) ' + page)\n",
    "    \n",
    "print('The top 5 pages according to the hits hub ranking are: ')\n",
    "for page in top_5_hits_hub_pages:\n",
    "    print (str(top_5_hits_hub_pages.index(page) + 1) + '.) ' + page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
